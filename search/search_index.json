{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#a-python-interface-to-lean-4","title":"A Python Interface to Lean 4","text":"Fast, scalable, and easy-to-use <p>Get Started View on GitHub</p>"},{"location":"#what-is-leanflow","title":"What is LeanFlow?","text":"<p>LeanFlow is an easy-to-use, scalable Python interface to Lean 4. It lets you run Lean code, interact with proofs, and evaluate formal statements from Python. Ideal for researchers in autoformalization, theorem proving, and experimentation.</p>"},{"location":"#fast-efficient","title":"\ud83d\ude80 Fast &amp; Efficient","text":"<p>LeanFlow is built for efficiency, allowing you to execute large-scale workloads simultaneously with minimal overhead.</p>"},{"location":"#easy-to-use","title":"\ud83d\ude0a Easy to Use","text":"<p>A small, Pythonic API that feels intuitive. Start running Lean in just a few lines of code.</p>"},{"location":"#flexible-deployment","title":"\ud83d\udd04 Flexible Deployment","text":"<p>Use interactive mode for local development or server mode for scalable experiments. Switch between them with a small config change.</p>"},{"location":"#evaluation-tools","title":"\ud83c\udfaf Evaluation Tools","text":"<p>Built-in metrics for autoformalization evaluation: TypeCheck, BEq+, LLM Grader, and more.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install leanflow\n</code></pre>"},{"location":"#get-started-in-3-lines","title":"Get Started in 3 Lines","text":"<p>The simplest way to run Lean from Python:</p> <pre><code>from leanflow import SyncREPL\n\nrepl = SyncREPL(lean_version=\"4.24.0\", require_mathlib=True)\nprint(repl.run(\"#eval 1 + 1\"))\n</code></pre> <p>That's it! LeanFlow automatically downloads Lean and sets up the environment.</p> <p>Built for Performance</p> <p>LeanFlow is built on <code>asyncio</code> for high-throughput parallel execution. The synchronous API shown above is perfect for getting started, scripts, and notebooks. For production workloads with parallel evaluation, see the async API.</p>"},{"location":"#two-ways-to-run-lean","title":"Two Ways to Run Lean","text":"\ud83d\udcbb Local      \ud83c\udf10 Server        Run Lean directly on your machine. Perfect for development and testing.  <pre><code>from leanflow import SyncREPL\n\nrepl = SyncREPL(lean_version=\"4.21.0\")\nresult = repl.run(\"def double (n : Nat) := 2 * n\\n#eval double 21\")\nprint(result)  # Environment(messages=..., data='42')\n</code></pre>  Connect to a remote LeanFlow server for shared environments and scalable evaluation.  <pre><code>from leanflow import SyncClient\n\nclient = SyncClient(base_url=\"http://localhost:8000\")\nresult = client.run(\"def double (n : Nat) := 2 * n\\n#eval double 21\")\nprint(result) # Environment(messages=..., data='42')\n</code></pre>  Start a server with: `leanflow-serve --config server.yaml`"},{"location":"#evaluation-metrics","title":"Evaluation Metrics","text":"<p>LeanFlow includes built-in metrics for autoformalization evaluation, including type checking and semantic equivalence checks:</p> <pre><code>from leanflow import TypeCheck, BEqPlus\n\nheader = \"import Mathlib\"\nthm1   = \"theorem infinite_primes (n : Nat) : \u2203 p, n &lt; p \u2227 Nat.Prime p := sorry\"\nthm2   = \"theorem infinite_primes_alt (k : Nat) : \u2203 q, k &lt; q \u2227 Nat.Prime q := sorry\"\n\n# Type checking\nmetric = TypeCheck(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(thm1, header=header)  \nprint(result) # True\n\n# Semantic equivalence (BEq+)\nmetric = BEqPlus(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(thm1, thm2, header=header)\nprint(result) # True\n</code></pre> <p>Run evaluations at scale with the CLI:</p> <pre><code>leanflow-eval --config config.yaml\n</code></pre> <p> Learn more about metrics</p>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p> Quickstart</p> <p>Get up and running in 5 minutes with our step-by-step guide</p> <p> Quickstart</p> </li> <li> <p> User Guide</p> <p>Learn about REPL, Server, and CLI in depth</p> <p> Documentation</p> </li> <li> <p> API Reference</p> <p>Complete reference for all classes and methods</p> <p> API Docs</p> </li> </ul>"},{"location":"#inspiration-acknowledgements","title":"Inspiration &amp; Acknowledgements","text":"<p>This project builds upon the incredible work of the Lean community. We are deeply grateful to the authors of the following projects, which directly inspired LeanFlow:</p> <ul> <li>LeanInteract by Auguste Poiroux</li> <li>Rethinking and Improving Autoformalization by Qi Liu</li> <li>Kimina by Project Numina</li> </ul> <p>We highly recommend checking out these projects!</p> <p>Special thanks to the Lean community, the contributors to Mathlib, and the authors of the Lean REPL, whose tools make this ecosystem possible.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: Report issues and contribute</li> <li>Discussions: Ask questions</li> </ul>"},{"location":"cli/","title":"Evaluation CLI","text":"<p>The LeanFlow Evaluation CLI (<code>leanflow-eval</code>) is a command\u2011line tool for running evaluation on autoformalization datasets. It's primary goal is facilitating research workloads.</p> <p>It allows you to run metrics like typecheck over large datasets of Lean statements. It handles parallelization, error logging, and results aggregation automatically.</p>"},{"location":"cli/#1-usage","title":"1. Usage","text":"<pre><code>leanflow-eval --config config.yaml\n</code></pre>"},{"location":"cli/#2-configuration-file","title":"2. Configuration File","text":"<p>The CLI is controlled entirely by a YAML configuration file.</p>"},{"location":"cli/#21-general-settings","title":"2.1 General Settings","text":"<pre><code># Input: Path to JSON file or HuggingFace dataset directory\ndata_path: ./data/examples.json\n\n# Output: Directory where results will be saved\noutput_path: ./results\n\n# Overwrite existing results (default: false)\noverwrite_all: false\n\n# Logging verbosity (debug, info, warning, error)\nlog_level: info\n\n# Devices to use (comma-separated list of device IDs)\ndevices: \"0,1\"\n</code></pre>"},{"location":"cli/#22-execution-backend","title":"2.2 Execution Backend","text":"<p>You must define how LeanFlow executes the code. You can either run Lean locally (spawning new processes) or connect to a running server.</p> LocalServer <pre><code>repl_config:\n  # Lean version to use (auto-downloaded if needed)\n  lean_version: \"4.24.0\"\n\n  # Optional: Path to a local Lean project\n  # project_path: /home/user/my-lean-project\n\n  # Timeout per statement (seconds)\n  timeout: 300\n\nuse_multiprocessing: true\n</code></pre> <p><pre><code>repl:\n  # URL of the LeanFlow server\n  base_url: \"http://localhost:8000\"\n\n  # Client-side timeout (seconds)\n  timeout: 300\n</code></pre> - You must start a LeanFlow server separately (see Client\u2013Server Architecture). - The CLI sends all Lean code to that server via the <code>Client</code>.</p>"},{"location":"cli/#23-metrics","title":"2.3 Metrics","text":"<p>Define which metrics to compute. You can run multiple metrics in a single pass.</p> <pre><code>metrics:\n  - typecheck\n  - beq_plus\n\n# Optional: Configuration for individual metrics \ntypecheck:\n  overwrite: true  # Overwrite results even if they exist\n</code></pre>"},{"location":"cli/#24-preprocessing-options","title":"2.4 Preprocessing Options","text":"<p>LLM output often lacks necessary imports. You can inject them globally using set_header.</p> <pre><code># Option 1: Replace all headers with a specific import block\nset_header: |\n  import Mathlib\n  open Nat\n\n# Option 2: Remove all headers\n# remove_header: true\n\n# Option 3: Extract headers from generated code\n# extract_header: true\n</code></pre> <ul> <li><code>set_header</code>: ensures all instances share the same imports and setup.</li> <li><code>remove_header</code>: strips all user\u2011provided headers.</li> <li><code>extract_header</code>: attempts to separate user code into header + body.</li> </ul>"},{"location":"cli/#3-data-format","title":"3. Data Format","text":"<p>The CLI expects either:</p> <ul> <li>a JSON file containing a list of objects, or</li> <li>a Hugging Face dataset directory.</li> </ul> <p>Each metric requires certain fields to be present in each data item.</p>"},{"location":"cli/#required-fields-by-metric","title":"Required Fields by Metric","text":"Metric Required Fields Description <code>typecheck</code> <code>formal_statement</code> Lean statement to type\u2011check <code>beq_plus</code> <code>formal_statement</code>, <code>formal_statement_generated</code> Ground truth and generated statement <code>beq_l</code> <code>formal_statement</code>, <code>formal_statement_generated</code> Ground truth and generated statement <code>equiv_rfl</code> <code>formal_conjecture</code>, <code>formal_conjecture_generated</code> Ground truth and generated conjecture <p>(Adjust names as needed to match your actual metric implementations.)</p>"},{"location":"cli/#example-json","title":"Example JSON","text":"<pre><code>[\n  {\n    \"formal_statement\": \"theorem add_zero_nat (n : Nat) : n + 0 = n := by rfl\",\n    \"formal_statement_generated\": \"theorem add_zero_nat (n : Nat) : 0 + n - 0 = n := by rfl\",\n    \"header\": \"import Mathlib\"\n  }\n]\n</code></pre>"},{"location":"cli/#4-output-format","title":"4. Output Format","text":"<p>Results are saved as JSON files in the <code>output_path</code>. The tool adds a boolean field for each metric run.</p> <p>Example output:</p> <pre><code>[\n  {\n    \"formal_statement\": \"...\",\n    \"formal_statement_generated\": \"...\",\n    \"typecheck\": true,\n    \"beq_plus\": false,\n  }\n]\n</code></pre>"},{"location":"cli/#5-custom-metrics","title":"5. Custom Metrics","text":"<p>You can plug in your own evaluation logic without modifying the LeanFlow source code.</p> <ul> <li> <p>Create a Python file (e.g., my_metric.py) with your metric class.</p> </li> <li> <p>Register it in the YAML config.</p> </li> </ul> <pre><code>metrics:\n  - my_custom_metric\n\nmy_metric:\n  source_file: ./metrics/my_metric.py\n  class_name: MyCustomMetric\n  my_parameter: 42\n</code></pre> <p>Your <code>MyCustomMetric</code> class should follow the expected interface used by the CLI (e.g. a <code>compute(...)</code> or similar method). See Custom Metrics for a full walkthrough.</p>"},{"location":"custom_metrics/","title":"Custom Metrics","text":"<p>LeanFlow is designed to be extensible. You can create your own evaluation metrics using <code>leanflow.metrics.Metric</code>.</p>"},{"location":"custom_metrics/#1-creating-a-logic-based-metric","title":"1. Creating a Logic-Based Metric","text":"<p>Let's build a <code>NoSorryMetric</code>. This metric will:</p> <ul> <li>Check if the generated code contains the string sorry.</li> <li>If it doesn't, verify that the code actually compiles (typechecks).</li> </ul>"},{"location":"custom_metrics/#11-implementation","title":"1.1. Implementation","text":"<p>Create a file named <code>my_metrics/no_sorry.py</code></p> <pre><code>from typing import Any, Dict\nfrom leanflow.metrics import Metric\n\nclass NoSorryMetric(Metric):\n    \"\"\"Checks that generated code doesn't contain 'sorry' and optionally typechecks.\"\"\"\n\n    def __init__(self, metric_config: dict[str, Any] = {}, **shared_dependencies):\n        super().__init__(metric_config, **shared_dependencies)\n        self.also_check_typecheck = metric_config.get(\"check_typecheck\", True)\n\n    async def run_check_async(self, example: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Check if the generated statement has no 'sorry' and optionally typechecks.\n\n        Returns:\n            Dict with 'has_sorry' and optionally 'typechecks' fields.\n        \"\"\"\n        runner, context = self.get_runner()\n\n        async with context:\n            statement = example.get(\"formal_statement_generated\", \"\")\n\n            # Text-based check\n            has_sorry = \"sorry\" in statement.lower()\n            result: Dict[str, Any] = {\"has_sorry\": has_sorry}\n\n            # Optionally also typecheck when there is no 'sorry'\n            if self.also_check_typecheck and not has_sorry:\n                env = await runner.run(statement)\n                has_error = any(m.severity == \"error\" for m in env.messages)\n                result[\"typecheck\"] = not has_error\n\n            return result\n</code></pre>"},{"location":"custom_metrics/#12-testing","title":"1.2. Testing","text":"<pre><code>import asyncio\nfrom my_metrics.no_sorry import NoSorryMetric  # adjust path\n\nasync def main():\n    metric = NoSorryMetric(\n        config={\"check_typecheck\": True},\n        repl_config={\"lean_version\": \"4.24.0\"},\n    )\n\n    example = {\n        \"formal_statement_generated\": \"theorem test : True := by sorry\"\n    }\n\n    result = await metric.run_check_async(example)\n    print(result)  # e.g. {'has_sorry': True}\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"custom_metrics/#2-creating-an-llm-as-a-judge-metric","title":"2. Creating an LLM-as-a-Judge Metric","text":"<p>You can also use an LLM to judge the output. This is common for \"soft\" evaluations, like judging the readability of proofs.</p> <pre><code>from typing import Any\nfrom leanflow import Metric\nimport openai  # or another async client\n\nclass LLMNoSorryMetric(Metric):\n    \"\"\"Use an LLM to review code that may contain 'sorry'.\"\"\"\n\n    def __init__(self, metric_config: dict[str, Any] = {}, **shared_dependencies):\n        super().__init__(metric_config, **shared_dependencies)\n        self.model = metric_config.get(\"model\", \"gpt-4\")\n\n        # Pull API configuration from shared dependencies\n        api_config = shared_dependencies.get(\"api_config\", {})\n        self.client = openai.AsyncOpenAI(api_key=api_config.get(\"api_key\"))\n\n    async def run_check_async(self, example: dict[str, Any]) -&gt; dict[str, Any]:\n        code = example.get(\"formal_statement_generated\", \"\")\n        has_sorry = \"sorry\" in code.lower()\n\n        # If there is no 'sorry', we can consider this trivially passing\n        if not has_sorry:\n            return {\"has_sorry\": False, \"llm_accepts\": True}\n\n        prompt = f\"\"\"Review the following Lean code that uses 'sorry':\n\n{code}\n\nIs this an acceptable use of 'sorry' in the context of a partially completed proof?\nReply with 'YES' if you consider it acceptable, otherwise 'NO'.\"\"\"\n\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n\n        answer = response.choices[0].message.content\n        llm_accepts = \"YES\" in answer.upper()\n\n        return {\n            \"has_sorry\": True,\n            \"llm_accepts\": llm_accepts,\n        }\n</code></pre>"},{"location":"custom_metrics/#3-integration-with-evaluation-cli","title":"3. Integration with Evaluation CLI","text":"<p>To use these metrics with <code>leanflow-eval</code>, you just need to update your YAML configuration file. You don't need to modify LeanFlow's source code.</p> <p><code>config.yaml</code>:</p> <pre><code>data_path: ./data/examples.json\noutput_path: ./results\n\nrepl_config:\n  lean_version: \"4.24.0\"\n\napi_config:\n  api_key:  &lt;KEY&gt;\n  base_url: &lt;URL&gt;\n\nmetrics:\n  - no_sorry\n  - sorry_judge\n\nno_sorry:\n  source_file: ./my_metrics/no_sorry.py\n  class_name: NoSorryMetric\n  check_typecheck: true\n\nsorry_judge:\n  source_file: ./my_metrics/sorry_judge.py\n  class_name: LLMJudgeMetric\n  model: \"deepseek-math\"\n</code></pre> <p>Run the evaluation:</p> <pre><code>leanflow-eval --config config.yaml\n</code></pre>"},{"location":"metrics/","title":"Evaluation Metrics","text":"<p>LeanFlow provides a suite of metrics to evaluate the correctness of LLM generated Lean code focusing on autoformalisation. </p>"},{"location":"metrics/#metrics-overview","title":"Metrics Overview","text":"Metric Description Key Method TypeCheck Verifies if code compiles without errors. checks for <code>repl</code> errors BEqPlus Strong bidirectional equivalence check using a suite of tactics. <code>exact?</code>, <code>simp</code>, <code>tauto</code>, <code>ring</code> BEqL Lightweight bidirectional equivalence check. <code>exact?</code> only EquivRfl Checks for definitional equality. <code>rfl</code> LLMGrader Semantic equivalence via backtranslation. LLM (Backtranslation + Judge) BEq Equivalence check augmented with LLM-generated tactics. <code>exact?</code> + LLM Generation ConJudge Verifies if a formal statement matches a formal conjecture. LLM (Judge)"},{"location":"metrics/#1-general-settings","title":"1. General Settings","text":"<p>Metrics are available under <code>leanflow.metrics</code> and follow a consistent pattern:</p> <ul> <li> <p>Instantiate the metric with a configuration (e.g., <code>repl_config</code> for local execution or <code>base_url</code> for server execution).</p> </li> <li> <p>Compute the metric on your statements.</p> </li> </ul> <p>There are two categories of metrics:</p> <ul> <li> <p>Interactive Metrics: Run on individual examples (strings).</p> </li> <li> <p>Batch Metrics: Run on lists of examples, often requiring LLM API access.</p> </li> </ul>"},{"location":"metrics/#2-interactive-metrics","title":"2. Interactive Metrics","text":""},{"location":"metrics/#21-typecheck","title":"2.1. TypeCheck","text":"<p>TypeCheck verifies whether a Lean statement is syntactically valid and compiles successfully in the given environment.</p> SynchronousAsynchronous <pre><code>from leanflow import TypeCheck\n\nstatement = \"theorem t : 1 + 1 = 2 := by rfl\"\n\nmetric = TypeCheck(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(statement)\nprint(result)\n</code></pre> <pre><code>import asyncio\nfrom leanflow import TypeCheck\n\nstatement = \"theorem t : 1 + 1 = 2 := by rfl\"\n\nasync def main():\n    metric = TypeCheck(repl_config={\"lean_version\": \"4.24.0\"})\n    result = await metric.run_check_async(statement)\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"metrics/#22-beqplus","title":"2.2. BEqPlus","text":"<p>BEq+ checks whether two Lean statements are bidirectionally equivalent. It attempts to prove <code>A \u2194 B</code> using a suite of tactics (<code>simp</code>, <code>tauto</code>, <code>ring</code>, <code>exact?</code>). These tactics require Mathlib.</p> <p>Source: Reliable Evaluation and Benchmarks for Statement Autoformalization (Poiroux et al., EMNLP 2025)</p> SynchronousAsynchronous <pre><code>from leanflow import BEqPlus\n\nthm1 = \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\"\nthm2 = \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\"\n\nmetric = BEqPlus(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(thm1, thm2, header=\"import Mathlib\")\nprint(result)\n</code></pre> <pre><code>import asyncio\nfrom leanflow import BEqPlus\n\nthm1 = \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\"\nthm2 = \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\"\n\nasync def main():\n    metric = BEqPlus(repl_config={\"lean_version\": \"4.24.0\"})\n    result = await metric.run_check_async(thm1, thm2, header=\"import Mathlib\")\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"metrics/#23-beql","title":"2.3. BEqL","text":"<p>BEqL is a lightweight variant of BEq+. It only uses the library search tactic (<code>exact?</code>) to check equivalence.</p> <p>Source: Reliable Evaluation and Benchmarks for Statement Autoformalization (Poiroux et al., EMNLP 2025)</p> SynchronousAsynchronous <pre><code>from leanflow import BEqL\n\nthm1 = \"theorem t1 (p q : Prop) : \u00ac(p \u2228 q) \u2194 \u00acp \u2227 \u00acq := by sorry\"\nthm2 = \"theorem t2 (p q : Prop) : \u00acp \u2227 \u00acq \u2194 \u00ac(q \u2228 p) := by sorry\"\n\nmetric = BEqL(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(thm1, thm2)\nprint(result)\n</code></pre> <pre><code>import asyncio\nfrom leanflow import BEqL\n\nthm1 = \"theorem t1 (p q : Prop) : \u00ac(p \u2228 q) \u2194 \u00acp \u2227 \u00acq := by sorry\"\nthm2 = \"theorem t2 (p q : Prop) : \u00acp \u2227 \u00acq \u2194 \u00ac(q \u2228 p) := by sorry\"\n\nasync def main():\n    metric = BEqL(repl_config={\"lean_version\": \"4.24.0\"})\n    result = await metric.run_check_async(thm1, thm2)\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"metrics/#24-equivrfl","title":"2.4. EquivRfl","text":"<p>EquivRfl checks whether two statements are definitionally equal.</p> <p>Source: Conjecturing: An Overlooked Step in Formal Mathematical Reasoning (Sivakumar et al., 2025)</p> SynchronousAsynchronous <pre><code>from leanflow import EquivRfl\n\nconjecture_1 = \"abbrev foo : Nat := 2\"\nconjecture_2 = \"abbrev bar : Nat := 1 + 1\"\n\nmetric = EquivRfl(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(conjecture_1, conjecture_2)\nprint(result)\n</code></pre> <pre><code>import asyncio\nfrom leanflow import EquivRfl\n\nconjecture_1 = \"abbrev foo : Nat := 2\"\nconjecture_2 = \"abbrev bar : Nat := 1 + 1\"\n\nasync def main():\n    metric = EquivRfl(repl_config={\"lean_version\": \"4.24.0\"})\n    result = await metric.run_check_async(conjecture_1, conjecture_2)\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"metrics/#3-llmasajudge-metrics","title":"3. LLM\u2011as\u2011a\u2011Judge Metrics","text":"<p>These metrics rely on external LLM APIs to judge correctness or semantic equivalence. They are useful when formal proof evaluation fails or is too strict.</p>"},{"location":"metrics/#31-llmgrader","title":"3.1. LLMGrader","text":"<p>LLMGrader performs semantic comparison via Back-Translation:</p> <ul> <li>Translate the Ground Truth Lean code back to Natural Language.</li> <li>Translate the Generated Lean code back to Natural Language.</li> <li>Ask an LLM Judge if the two Natural Language statements have the same meaning.</li> </ul> <p>Source: FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models  (Yu et al., 2025)</p> <pre><code>from leanflow import LLMGrader\n\ndata = {\n    \"formal_statement\": \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\",\n    \"formal_statement_generated\": \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\"\n}\n\nmetric = LLMGrader(\n    api_config={\"base_url\": \"&lt;URL&gt;\", \"api_key\": \"&lt;KEY&gt;\"},\n    backtranslation={\"model\": \"deepseek-math\"},\n    comparison={\"model\": \"gpt-4\"},\n)\n\nresult = metric.compute_batch([data])\nprint(result)\n</code></pre>"},{"location":"metrics/#32-beq","title":"3.2. BEq","text":"<p>BEq enhances bidirectional equivalence checking by using <code>exact?</code> and an LLM to generate proof tactics.</p> <p>Source: Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach (Liu et al., 2024)</p> <pre><code>from leanflow import BEq\n\ndata = {\n    \"header\": \"import Mathlib\"\n    \"formal_statement\": \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\",\n    \"formal_statement_generated\": \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\",\n}\n\nmetric = BEq(\n    api_config={\"base_url\": \"&lt;URL&gt;\", \"api_key\": \"&lt;KEY&gt;\"},\n    tactic_generator={\"model\": \"deepseek-math\"},\n    repl_config={\"lean_version\": \"v4.24.0\"}\n)\n\nresult = metric.compute_batch([data])\nprint(result)\n</code></pre>"},{"location":"metrics/#33-conjudge","title":"3.3. ConJudge","text":"<p>ConJudge evaluates whether a generated formal statement correctly captures the semantics of a formal conjecture. It uses an LLM as a judge.</p> <p>Source: Conjecturing: An Overlooked Step in Formal Mathematical Reasoning (Sivakumar et al., 2025)</p> <pre><code>from leanflow import ConJudge\n\ndata = {\n    \"header\": \"import Mathlib\",\n    \"formal_conjecture\": \"abbrev conjecture : \u2115 : 13\",\n    \"formal_statement\": \"theorem hackmath_4 : IsLeast {n | \u2200 f : Fin n \u2192 Fin 12, \u2203 a b, f a = f b} ((conjecture) : \u2115 ) := by sorry\",\n    \"formal_statement_generated\": \"theorem hackmath_4 : IsLeast {n | \u2200 f : Fin n \u2192 Fin 12, \u2203 a b, f a = f b} (26 / 2 : \u2115 ) := by sorry\",\n}\n\nmetric = ConJudge(\n    api_config={\"base_url\": \"&lt;URL&gt;\", \"api_key\": \"&lt;KEY&gt;\"},\n    comparison={\"model\": \"gpt-4\"}\n)\n\nresult = metric.compute_batch([data])\nprint(result)\n</code></pre>"},{"location":"metrics/#4-batch-evaluation","title":"4. Batch Evaluation","text":"<p>For evaluating large datasets using these metrics, use the Evaluation CLI (<code>leanflow-eval</code>). It handles parallel execution, error logging, and result aggregation automatically.</p> <p>See the Evaluation CLI Guide for configuration details and usage examples.</p> <pre><code>leanflow-eval --config config.yaml\n</code></pre>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>Welcome to LeanFlow! This guide will walk you through installation and running your first Lean 4 commands from Python.</p>"},{"location":"quickstart/#1-installation","title":"1. Installation","text":""},{"location":"quickstart/#install-leanflow","title":"Install LeanFlow","text":"<pre><code>pip install leanflow\n</code></pre>"},{"location":"quickstart/#install-lean-4","title":"Install Lean 4","text":"<p>LeanFlow requires Lean 4 to be installed on your system. Check out the official Lean 4 installation guide for installation details.</p> <pre><code>curl https://elan.lean-lang.org/elan-init.sh -sSf | sh\n</code></pre>"},{"location":"quickstart/#2-your-first-command","title":"2. Your First Command","text":"<p>LeanFlow provides both Synchronous and Asynchronous interfaces. Use the toggle below to see the pattern that fits your project.</p> <p>First Run Note</p> <p>LeanFlow will automatically fetch the requested version (e.g., <code>v4.24.0</code>) and configure the environment the first time you run code.</p> SynchronousAsynchronous <p>Best for: Scripts, Jupyter notebooks, and interactive exploration.</p> <pre><code>from leanflow import SyncREPL\n\n# A simple theorem with a missing proof ('sorry')\ntheorem = \"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\"\n\nwith SyncREPL(lean_version=\"4.24.0\") as repl:\n    env = repl.run(theorem)\n    print(env)\n</code></pre> <p>Best for: High-throughput research and production environments.</p> <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n\n    # A simple theorem with a missing proof ('sorry')\n    theorem = \"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\"\n\n    async with REPL(lean_version=\"4.24.0\") as repl:\n        result = await repl.run(theorem)\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#3-understanding-results","title":"3. Understanding Results","text":"<p>When you run a command, LeanFlow returns an <code>Environment</code> object. This object tells you everything about the current state of Lean, including what is left to prove and any errors that occurred.</p> <ul> <li><code>env</code>: Environment ID</li> <li><code>messages</code>: List of Lean messages (errors, warnings, info)</li> </ul> <pre><code>result = repl.run(\"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\")\n\nprint(result.goals) \n# Output: ['n : Nat\\n\u22a2 n + 0 = n']\n\nfor msg in result.messages:\n    print(f\"[{msg.severity}] {msg.data}\")\n# Output: [warning] declaration uses 'sorry'\n\nprint(result.sorries)\n# Output: [Sorry(pos=..., goal='n : Nat\\n\u22a2 n + 0 = n')]\n</code></pre> Component Description goals A list of current tactical goals (what is left to prove). messages Compiler output, including errors, warnings, and results. sorries Specific data on sorry placeholders, including their exact position and the goal they were meant to solve."},{"location":"quickstart/#4-using-a-server","title":"4. Using a Server","text":"<p>While running LeanFlow locally is great for testing, you may want to use the Server Mode for larger projects.</p>"},{"location":"quickstart/#41-why-use-a-server","title":"4.1. Why use a Server?","text":"<ul> <li>Instant Startup: Lean takes time to start up and import libraries. A server keeps the environment \"warm\" in the background, making your scripts respond instantly.</li> <li>Consistency: Ensure your entire team is running against the exact same Lean version and configuration.</li> <li>Offload Computation: Run the Lean server on a powerful machine (e.g., a cloud instance).</li> </ul>"},{"location":"quickstart/#42-create-a-config-file-serveryaml","title":"4.2. Create a config file (<code>server.yaml</code>):","text":"<pre><code>server:\n  host: localhost\n  port: 8000\nrepl:\n  lean_version: \"4.24.0\"\n</code></pre>"},{"location":"quickstart/#43-start-the-server","title":"4.3. Start the server:","text":"<pre><code>leanflow-serve --config server.yaml\n</code></pre>"},{"location":"quickstart/#44-connect-from-python","title":"4.4. Connect from Python:","text":"<p>Now, instead of starting a local REPL, connect to your running server:</p> SynchronousAsynchronous <pre><code>from leanflow import SyncClient\n\nclient = SyncClient(base_url=\"http://localhost:8000\")\nresult = client.run(\"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\")\nprint(result)\n</code></pre> <pre><code>import asyncio\nfrom leanflow import Client\n\nasync def main():\n    async with Client(base_url=\"http://localhost:8000\") as client:\n        result = await client.run(\"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\")\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li> <p> REPL Guide</p> <p>Configuration, state management, custom projects, and troubleshooting</p> <p> REPL Documentation</p> </li> <li> <p> Server Mode</p> <p>Running LeanFlow as a service for shared environments</p> <p> Server Guide</p> </li> <li> <p> Evaluation Metrics</p> <p>TypeCheck, BEq+, LLM Grader for autoformalization research</p> <p> Metrics</p> </li> <li> <p> API Reference</p> <p>Complete reference for all classes and methods</p> <p> API Docs</p> </li> </ul>"},{"location":"repl/","title":"Interactive REPL","text":"<p>The Interactive REPL provides a persistent interface for executing Lean 4 code.</p> <p>It allows you to run Lean commands, inspect the proof state, and maintain a session where definitions and theorems persist between calls. This is ideal for scripting Lean interactions, testing tactics, or building tools that require fine-grained control over the Lean compiler.</p>"},{"location":"repl/#1-basic-usage","title":"1. Basic Usage","text":"<p>Choose the API that fits your needs:</p> SynchronousAsynchronous <p>Best for: Debugging, scripts, and data exploration.</p> <p>The synchronous API provides a simple, convenient way to interact with Lean without <code>async</code>/<code>await</code> boilerplate.</p> <pre><code>from leanflow import SyncREPL\n\nwith SyncREPL(lean_version=\"4.24.0\") as repl:\n    result = repl.run(\"#check 1 + 1\")\n    print(result)\n</code></pre> <p>Best for: High-performance applications and web services.</p> <p>The asynchronous API uses Python's <code>asyncio</code> to handle multiple Lean operations concurrently, essential for performance at scale.</p> <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n    async with REPL(lean_version=\"4.24.0\") as repl:\n        result = await repl.run(\"#check 1 + 1\")\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"repl/#2-understanding-the-environment","title":"2. Understanding the Environment","text":"<p>Every time you run a command, LeanFlow returns an <code>Environment</code> object. This object captures the state of the compiler after your code runs, including any errors, warnings, or remaining proof goals.</p>"},{"location":"repl/#21-inspecting-unfinished-proofs","title":"2.1 Inspecting Unfinished Proofs","text":"<p>When a proof uses <code>sorry</code>, Lean accepts the syntax but reports the missing logic as a \"sorry\" placeholder. You can inspect this to see exactly what remains to be proved.</p> SynchronousAsynchronous <pre><code>from leanflow import SyncREPL\n\ncode = \"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\"\n\nwith SyncREPL(lean_version=\"4.24.0\") as repl:\n    env = repl.run(code)\n    print(f\"Goals:   {env.goals}\")\n    print(f\"Sorries: {env.sorries}\")\n</code></pre> <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n    code = \"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\"\n\n    async with REPL(lean_version=\"4.24.0\") as repl:\n        env = await repl.run(code)\n        print(f\"Goals:   {env.goals}\")\n        print(f\"Sorries: {env.sorries}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"repl/#22-verifying-complete-proofs","title":"2.2 Verifying Complete Proofs","text":"<p>When a proof is valid and complete (e.g., using <code>rfl</code>), the environment does not contain any error messages or open goal states.</p> SynchronousAsynchronous <pre><code>from leanflow import SyncREPL\n\ncode = \"theorem add_zero_nat (n : Nat) : n + 0 = n := by rfl\"\n\nwith SyncREPL(lean_version=\"4.24.0\") as repl:\n    env = repl.run(code)\n\n    # A successful proof has empty lists\n    assert not env.goals\n    assert not env.sorries\n    print(\"Proof complete!\")\n</code></pre> <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n    code = \"theorem add_zero_nat (n : Nat) : n + 0 = n := by rfl\"\n\n    async with REPL(lean_version=\"4.24.0\") as repl:\n        env = await repl.run(code)\n\n        # A successful proof has empty lists\n        assert not env.goals\n        assert not env.sorries\n        print(\"Proof complete!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"repl/#3-state-management","title":"3. State Management","text":"<p>The REPL is stateful. Definitions and theorems you execute are remembered for the duration of the session, allowing you to build up complex environments step-by-step.</p>"},{"location":"repl/#31-explicit-chaining","title":"3.1 Explicit Chaining","text":"<p>If you are running commands individually (e.g., inside a loop or conditional logic), you can manually pass the <code>env</code> to the next run command.</p> SynchronousAsynchronous <pre><code>from leanflow import SyncREPL\n\nwith SyncREPL(lean_version=\"4.24.0\") as repl:\n    env1 = repl.run(\"def double (n : Nat) := 2 * n\")\n    result = repl.run(\"#eval double 21\", env=env1)\n    print(result.messages[-1].data)  # 42\n</code></pre> <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n    async with REPL(lean_version=\"4.24.0\") as repl:\n        env1 = await repl.run(\"def double (n : Nat) := 2 * n\")\n        result = await repl.run(\"#eval double 21\", env=env1)\n        print(result.messages[-1].data) # 42\n\nasyncio.run(main())\n</code></pre>"},{"location":"repl/#32-implicit-chaining","title":"3.2 Implicit Chaining","text":"<p>You can pass a list of commands to <code>run_list</code>. The REPL automatically propagates the environment state from one command to the next.</p> SynchronousAsynchronous <pre><code>from leanflow import SyncREPL\n\nwith SyncREPL(lean_version=\"4.24.0\") as repl:\n    results = repl.run_list([\n        \"def double (n : Nat) := 2 * n\",\n        \"#eval double 21\"\n    ])\n    print(results[-1].messages[-1].data)  # 42\n</code></pre> <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n    async with REPL(lean_version=\"4.24.0\") as repl:\n        results = await repl.run_list([\n            \"def double (n : Nat) := 2 * n\",\n            \"#eval double 21\"\n        ])\n    print(results[-1].messages[-1].data)  # 42\n\nasyncio.run(main())\n</code></pre>"},{"location":"repl/#4-configuration","title":"4. Configuration","text":"<p>You can customize the Lean environment by passing arguments to <code>REPL</code>.</p> <p>Typical options include:</p> <ul> <li><code>lean_version</code>: Lean version to use, e.g. <code>\"v4.24.0\"</code>.</li> <li><code>require_mathlib</code>: whether to load Mathlib (default may be <code>True</code>).</li> <li><code>project_path</code>: path to an existing Lean project.</li> <li><code>timeout</code>: optional timeout (seconds) for each command.</li> </ul> <p>Example:</p> <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n    async with REPL(\n        lean_version=\"4.24.0\",\n        require_mathlib=True,\n        timeout=300,\n    ) as repl:\n        env = await repl.run(\"import Mathlib\\n#check Nat\")\n        print(env)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"repl/#5-using-custom-lean-projects","title":"5. Using Custom Lean Projects","text":"<p>By default, LeanFlow handles everything for you. It automatically downloads Lean and Mathlib into the <code>$HOME/.leanflow</code> directory. </p> <p>How to Load a Custom Project</p> <p>Simply provide the absolute path to your project's root directory using the <code>project_path</code> argument.</p> <p>If you are working on a specific theorem proving repository, you likely have a local folder with its own dependencies and configuration file. You can configure LeanFlow to run inside this existing project context. This allows you to:</p> <ul> <li>Import local modules (e.g., <code>import MyProject.Chapter1</code>).</li> <li>Use custom dependencies defined in your lakefile.</li> <li>Ensure consistency with your local development environment.</li> </ul>"},{"location":"repl/#5-environment-diagnostics","title":"5. Environment Diagnostics","text":"<p>If you encounter issues with toolchains or paths, you can inspect the internal configuration using the <code>EnvironmentManager</code>.</p> <pre><code>from leanflow import EnvironmentManager\n\nmanager = EnvironmentManager({\"lean_version\": \"4.24.0\"})\nprint(manager.diagnose_pretty())\n</code></pre> <p>This outputs: <pre><code>=== LeanFlow Environment Diagnostics ===\n\nPython: 3.11.0\nPlatform: linux\nCPU count: 64\n\n--- Paths ---\nHome: /home/user\nBase path: /home/user/.leanflow (exists: True, writable: True)\n\n--- Tools ---\nLake: Lake version 5.0.0-src+797c613 (Lean version 4.24.0)\nGit: git version 2.52.0\nElan: elan 4.1.2\n\n--- Existing Environments ---\n  - lean-v4.24.0_mathlib\n</code></pre></p>"},{"location":"server/","title":"Client-Server Architecture","text":"<p>LeanFlow also supports a client\u2013server model:</p> <p>In this architecture, a centralized server manages the heavy lifting (Lean runtime, Mathlib compilation, and state), while lightweight clients send commands and receive results over HTTP.</p>"},{"location":"server/#1-server-configuration","title":"1. Server Configuration","text":"<p>The server hosts a persistent Lean environment behind a simple API. You configure it using a YAML file.</p>"},{"location":"server/#configuration-serveryaml","title":"Configuration (<code>server.yaml</code>)","text":"<pre><code>server:\n  host: 0.0.0.0\n  port: 8000\n  stateless: false    # Set to 'true' to disable persistent environments\n  # max_workers: 20   # optional \n\nrepl:\n  lean_version: \"v4.24.0\"\n  require_mathlib: true\n  timeout: 300\n  # project_path: /path/to/lean-project  # optional\n\n  # \"Warm Start\" Header: These commands run once at startup. All new environments inherit this state.\n  header: |\n    import Mathlib\n</code></pre>"},{"location":"server/#starting-the-server","title":"Starting the Server","text":"<pre><code>leanflow-serve --config server.yaml\n</code></pre>"},{"location":"server/#stateless-mode","title":"Stateless Mode","text":"<p>By setting stateless: true, you configure the server to discard environments immediately after execution. The <code>env</code> field in the response will always be <code>None</code>.</p>"},{"location":"server/#warm-start-with-header-field","title":"Warm start with <code>header</code> field:","text":"<p>The header block allows you to define a \"base state\" that is shared by everyone. </p> <ul> <li> <p>Efficiency: Imports like <code>Mathlib</code> take time to load. By putting them in the header, they are loaded only once when the server starts.</p> </li> <li> <p>Consistency: New commands are executed with the exact same imports and namespaces opened, ensuring consistent behavior across experiments.</p> </li> </ul>"},{"location":"server/#2-client-usage","title":"2. Client Usage","text":"<p>The <code>Client</code> class provides a Pythonic wrapper for the server's API. It mirrors the REPL interface, making it easy to switch between local and remote execution.</p>"},{"location":"server/#basic-connection","title":"Basic Connection","text":"SynchronousAsynchronous <p>Best for: Scripts, Jupyter notebooks, and simple integrations.</p> <pre><code>from leanflow import SyncClient\n\n# Connect to the running server\nwith SyncClient(base_url=\"http://localhost:8000\") as client:\n\n    if client.status():\n        print(\"Server is connected!\")\n\n    result = client.run(\"#check 1 + 1\")\n    print(result)\n</code></pre> <p>Best for: High-performance applications and web services.</p> <pre><code>import asyncio\nfrom leanflow import Client\n\nasync def main():\n    async with Client(base_url=\"http://localhost:8000\") as client:\n        if await client.status():\n            print(\"Server is connected!\")\n\n        result = await client.run(\"#check 1 + 1\")\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"server/#3-environment-management","title":"3. Environment Management","text":"<p>Environments are persistent on the server when it's configured as stateful. You manage the states using Environment IDs.</p> SynchronousAsynchronous <pre><code>from leanflow import SyncClient\n\nwith SyncClient(base_url=\"http://localhost:8000\") as client:\n    # 1. Define in a fresh environment\n    env1 = client.run(\"def x : Nat := 42\")\n\n    # 2. Extend that environment (pass env ID)\n    env2 = client.run(\"def y : Nat := x + 1\", env=env1)\n\n    # 3. Evaluate in the extended environment\n    result = client.run(\"#eval y\", env=env2)\n    print(result.messages[-1].data)  # \"43\"\n</code></pre> <pre><code>import asyncio\nfrom leanflow import Client\n\nasync def main():\n    async with Client(base_url=\"http://localhost:8000\") as client:\n        # 1. Define in a fresh environment\n        env1 = await client.run(\"def x : Nat := 42\")\n\n        # 2. Extend that environment (pass env ID)\n        env2 = await client.run(\"def y : Nat := x + 1\", env=env1)\n\n        # 3. Evaluate in the extended environment\n        result = await client.run(\"#eval y\", env=env2)\n        print(result.messages[-1].data)  # \"43\"\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"server/#4-api-endpoints","title":"4. API Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/status</code> Check server status (returns <code>{\"status\": \"ok\"}</code>) <code>POST</code> <code>/run</code> Execute a Lean command"},{"location":"server/#post-run","title":"POST /run","text":"<p>Execute a Lean command.</p> <p>Request Body: <pre><code>{\n  \"command\": \"#check 1 + 1\",\n  \"env\": null\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"result\": {\n    \"env\": 1,\n    \"goals\": [],\n    \"messages\": [],\n    \"sorries\": [],\n  }\n}\n</code></pre></p>"},{"location":"api/cli/","title":"Evaluation CLI Reference","text":"<p>The <code>leanflow-eval</code> command-line tool runs evaluation metrics on autoformalization datasets.</p>"},{"location":"api/cli/#command-syntax","title":"Command Syntax","text":"<pre><code>leanflow-eval --config &lt;path_to_config.yaml&gt;\n</code></pre> <p>Or using the Python module:</p> <pre><code>python -m leanflow.evaluate_cli --config &lt;path_to_config.yaml&gt;\n</code></pre>"},{"location":"api/cli/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/cli/#top-level-options","title":"Top-Level Options","text":"Option Type Required Description <code>data_path</code> string Yes Path to input data (JSON file or HuggingFace dataset) <code>output_path</code> string Yes Directory for output results <code>overwrite_all</code> bool No Overwrite existing results (default: <code>false</code>) <code>log_level</code> string No Logging level: <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code> <code>use_multiprocessing</code> bool No Enable parallel processing (default: <code>true</code>) <code>devices</code> string No CUDA devices, e.g., <code>\"0,1\"</code>"},{"location":"api/cli/#repl-configuration","title":"REPL Configuration","text":"<p>Configure how LeanFlow connects to Lean:</p> <pre><code># Local mode\nrepl_config:\n  lean_version: \"4.21.0\"\n  require_mathlib: true\n  timeout: 300\n  # project_path: /path/to/project  # Optional\n\n# OR Server mode\nrepl_config:\n  base_url: \"http://localhost:8000\"\n  timeout: 300\n</code></pre> Option Type Description <code>lean_version</code> string Lean version (local mode) <code>require_mathlib</code> bool Include Mathlib (local mode) <code>project_path</code> string Path to Lean project (local mode) <code>base_url</code> string Server URL (server mode) <code>timeout</code> int Command timeout in seconds"},{"location":"api/cli/#metrics-configuration","title":"Metrics Configuration","text":"<p>List metrics to run and their individual configurations:</p> <pre><code>metrics:\n  - typecheck\n  - beq_plus\n  - my_custom_metric\n\ntypecheck:\n  overwrite: true\n\nmy_custom_metric:\n  source_file: ./my_metrics/custom.py\n  class_name: MyCustomMetric\n  custom_param: 42\n</code></pre>"},{"location":"api/cli/#built-in-metrics","title":"Built-in Metrics","text":"Metric Description Required Fields <code>typecheck</code> Checks if statement typechecks <code>formal_statement</code> <code>beq_plus</code> Behavioral equivalence (BEq+) <code>formal_statement</code>, <code>formal_statement_generated</code> <code>beq_l</code> Logical equivalence (BEqL) <code>formal_statement</code>, <code>formal_statement_generated</code> <code>equiv_rfl</code> Reflexive equivalence <code>formal_conjecture</code>, <code>formal_conjecture_generated</code>"},{"location":"api/cli/#custom-metrics","title":"Custom Metrics","text":"<p>Load custom metrics from Python files:</p> <pre><code>my_metric:\n  source_file: /absolute/path/to/metric.py\n  class_name: MyMetricClass\n  # Additional params passed to __init__\n  param1: value1\n</code></pre>"},{"location":"api/cli/#preprocessing-options","title":"Preprocessing Options","text":"<p>Control how Lean code is preprocessed:</p> <pre><code># Replace all headers\nset_header: |\n  import Mathlib\n  open Nat\n\n# OR remove headers\nremove_header: true\n\n# OR extract headers from code\nextract_header: true\n</code></pre>"},{"location":"api/cli/#llm-configuration","title":"LLM Configuration","text":"<p>For LLM-based metrics:</p> <pre><code>api_config:\n  base_url: \"https://api.openai.com/v1\"\n  api_key: \"sk-...\"\n\nsampling_params:\n  temperature: 0.7\n  max_tokens: 1024\n</code></pre>"},{"location":"api/cli/#complete-example","title":"Complete Example","text":"<pre><code># config.yaml\ndata_path: ./data/test_set.json\noutput_path: ./results\nlog_level: info\nuse_multiprocessing: true\n\nrepl_config:\n  lean_version: \"4.21.0\"\n  require_mathlib: true\n  timeout: 300\n\nmetrics:\n  - typecheck\n  - beq_plus\n\ntypecheck:\n  overwrite: false\n\nbeq_plus:\n  overwrite: false\n</code></pre>"},{"location":"api/cli/#output-format","title":"Output Format","text":"<p>Results are saved as JSON in <code>output_path</code>:</p> <pre><code>[\n  {\n    \"formal_statement\": \"theorem t : 1 + 1 = 2 := rfl\",\n    \"formal_statement_generated\": \"theorem t : 1 + 1 = 2 := by rfl\",\n    \"typecheck\": true,\n    \"beq_plus\": true\n  }\n]\n</code></pre>"},{"location":"api/cli/#see-also","title":"See Also","text":"<ul> <li>Evaluation Metrics \u2013 User guide for metrics</li> <li>Custom Metrics \u2013 Creating custom metrics</li> </ul>"},{"location":"api/client/","title":"Client API Reference","text":"<p>The <code>Client</code> module provides an interface for interacting with a LeanFlow Server.</p> <p>It abstracts the HTTP communication, allowing you to treat a remote Lean instance almost exactly like a local <code>REPL</code> instance. It automatically handles connection pooling, request serialization, and error parsing.</p> SynchronousAsynchronous <p>The <code>SyncClient</code> class is a blocking wrapper around the asynchronous client.</p> <p>The <code>Client</code> class is the native asynchronous implementation, built on top of <code>httpx</code>.</p>"},{"location":"api/client/#leanflow.SyncClient","title":"<code>leanflow.SyncClient</code>","text":"<p>Synchronous wrapper around the Client class. Provides a blocking API for interacting with a LeanFlow server without needing to use async/await.</p> Example <pre><code>from leanflow import SyncClient\n\nwith SyncClient(base_url=\"http://localhost:8000\") as client:\n    result = client.run(\"#eval 1 + 1\")\n    print(result)\n</code></pre>"},{"location":"api/client/#leanflow.SyncClient.__init__","title":"<code>__init__(base_url, timeout=300)</code>","text":"<p>Initialize the SyncClient with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>URL of the LeanFlow server.</p> required <code>timeout</code> <code>int</code> <p>Request timeout in seconds.</p> <code>300</code>"},{"location":"api/client/#leanflow.SyncClient.run","title":"<code>run(command, env=None)</code>","text":"<p>Run a Lean command synchronously via the server.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>A single command string.</p> required <code>env</code> <code>Optional[int]</code> <p>Optional environment ID to run in.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>Environment or LeanError.</p>"},{"location":"api/client/#leanflow.SyncClient.run_list","title":"<code>run_list(commands, env=None)</code>","text":"<p>Run Lean commands synchronously via the server.</p> <p>Parameters:</p> Name Type Description Default <code>commands</code> <code>list[str]</code> <p>A list of command strings.</p> required <code>env</code> <code>Optional[int]</code> <p>Optional environment ID to run in.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Union[Environment, LeanError]]</code> <p>List of results (Environment or LeanError).</p>"},{"location":"api/client/#leanflow.SyncClient.status","title":"<code>status()</code>","text":"<p>Check server status synchronously.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the server is running and accessible, False otherwise.</p>"},{"location":"api/client/#leanflow.SyncClient.close","title":"<code>close()</code>","text":"<p>Close the client connection and clean up resources.</p>"},{"location":"api/client/#leanflow.Client","title":"<code>leanflow.Client</code>","text":"<p>An asynchronous client for interacting with a remote LeanFlow Server.</p> Example <pre><code>import asyncio\nfrom leanflow import Client\n\nasync def main():\n    async with Client(\"http://localhost:8000\") as client:\n        result = await client.run(\"#eval 1 + 1\")\n        print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/client/#leanflow.Client.__init__","title":"<code>__init__(base_url, timeout=None, max_connections=1000, max_keepalive_connections=100, log_dir=None, log_level=None)</code>","text":"<p>Initializes the Client.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>The base URL of the leanflow-server.</p> required <code>timeout</code> <code>Optional[int]</code> <p>Request timeout in seconds.</p> <code>None</code> <code>max_connections</code> <code>int</code> <p>Maximum number of connections.</p> <code>1000</code> <code>max_keepalive_connections</code> <code>int</code> <p>Maximum number of keepalive connections.</p> <code>100</code> <code>log_dir</code> <code>Optional[Union[str, Path]]</code> <p>Explicit directory to save log files.</p> <code>None</code> <code>log_level</code> <code>Optional[str]</code> <p>The logging level (e.g., \"INFO\", \"DEBUG\").</p> <code>None</code>"},{"location":"api/client/#leanflow.Client.run","title":"<code>run(command, env=None)</code>  <code>async</code>","text":"<p>Executes a single Lean command.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The Lean command to execute.</p> required <code>env</code> <code>Optional[Union[Environment, int]]</code> <p>The environment ID to run in.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>The result of the command.</p>"},{"location":"api/client/#leanflow.Client.run_list","title":"<code>run_list(commands, env=None)</code>  <code>async</code>","text":"<p>Executes a list of commands sequentially, in the same environment and stopping on the first error.</p> <p>Parameters:</p> Name Type Description Default <code>commands</code> <code>list[str]</code> <p>A list of commands.</p> required <code>env</code> <code>Optional[Union[Environment, int]]</code> <p>The environment to run the commands in.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Union[Environment, LeanError]]</code> <p>List of results.</p>"},{"location":"api/client/#leanflow.Client.status","title":"<code>status()</code>  <code>async</code>","text":"<p>Checks if the remote server is running and accessible.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the server returns status 'ok', False otherwise.</p>"},{"location":"api/client/#leanflow.Client.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes the underlying HTTP client.</p>"},{"location":"api/errors/","title":"Error Types","text":"<p>LeanFlow provides structured error types for robust error handling. All errors inherit from the base <code>LeanFlowError</code>.</p>"},{"location":"api/errors/#quick-reference","title":"Quick Reference","text":"Error When It's Raised Key Attributes <code>LeanBuildError</code> Compilation of Lean or Lake dependencies fails. <code>message</code>, <code>stderr</code>, <code>return_code</code> <code>LeanTimeoutError</code> An operation exceeds the configured timeout limit. <code>message</code>, <code>timeout</code>, <code>operation</code> <code>LeanHeaderError</code> The startup header script fails to execute. <code>message</code>, <code>header</code>, <code>lean_error</code> <code>LeanConnectionError</code> The Client connection to the server fails. <code>message</code> <code>LeanMemoryError</code> The REPL exceeds configured memory limits. <code>message</code> <code>LeanValueError</code> Invalid arguments are passed to a function. <code>message</code> <code>LeanEnvironmentError</code> Errors raised when creating or locating the Lean environment. <code>message</code> <code>LeanServerError</code> Issues raised by the LeanFlow server. <code>message</code>"},{"location":"api/errors/#handling-errors","title":"Handling Errors","text":"<pre><code>from leanflow import SyncREPL, LeanBuildError, LeanTimeoutError, LeanEnvironmentError, LeanValueError\n\ntry:\n    with SyncREPL(lean_version=\"4.24.0\") as repl:\n        result = repl.run(\"#check 1 + 1\")\n        print(result)\nexcept LeanBuildError as e:\n    print(f\"Build failed: {e.message}\")\nexcept LeanTimeoutError as e:\n    print(f\"Timed out: {e.operation}\")\nexcept LeanEnvironmentError as e:\n    print(f\"Environment error: {e.message}\")\nexcept LeanValueError as e:\n    print(f\"Invalid argument: {e.message}\")\n</code></pre>"},{"location":"api/errors/#suppressing-header-errors","title":"Suppressing Header Errors","text":"<p>By default, header failures raise <code>LeanHeaderError</code>. To log warnings instead:</p> <pre><code>with SyncREPL(\n    lean_version=\"4.24.0\",\n    header=\"import Mathlib\",\n    fail_on_header_error=False  # Logs warning instead\n) as repl:\n    ...\n</code></pre>"},{"location":"api/errors/#diagnosing-environment-issues","title":"Diagnosing Environment Issues","text":"<p>For setup problems (especially on HPC clusters):</p> <pre><code>from leanflow import EnvironmentManager\n\nmanager = EnvironmentManager({\"lean_version\": \"4.24.0\"})\nprint(manager.diagnose_pretty())\n</code></pre>"},{"location":"api/metrics/","title":"Metrics API Reference","text":"<p>The Metrics module provides a suite of tools for evaluating formal mathematics. These classes allow you to assess the syntactic correctness, equivalence, and quality of Lean 4 code, ranging from simple typechecking to semantic comparisons using LLMs.</p>"},{"location":"api/metrics/#base-metric","title":"Base Metric","text":"MetricBatchMetric"},{"location":"api/metrics/#leanflow.Metric","title":"<code>leanflow.Metric</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for stateless metrics.</p> <p>The <code>compute</code> method is synchronous, making it compatible with multiprocessing.</p>"},{"location":"api/metrics/#leanflow.Metric.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns the snake_case name of the metric class.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the metric.</p>"},{"location":"api/metrics/#leanflow.Metric.__init__","title":"<code>__init__(metric_config={}, repl_config={}, client=None, **shared_dependencies)</code>","text":"<p>Initializes the metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary for the metric.</p> <code>{}</code> <code>repl_config</code> <code>dict[str, Any]</code> <p>Configuration for REPL/Client (e.g. {'lean_version': ...}).</p> <code>{}</code> <code>client</code> <code>Optional[Client]</code> <p>Existing client instance to share.</p> <code>None</code> <code>**shared_dependencies</code> <code>Any</code> <p>Other shared dependencies.</p> <code>{}</code>"},{"location":"api/metrics/#leanflow.Metric.compute","title":"<code>compute(*args, **kwargs)</code>","text":"<p>Synchronously computes the metric for a single example.</p> <p>This method is the entrypoint for a multiprocessing worker. It calls the REPL asynchronously and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments for the metric check.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the metric check.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the metric computation.</p>"},{"location":"api/metrics/#leanflow.Metric.get_runner","title":"<code>get_runner()</code>","text":"<p>Returns the appropriate LeanRunner and context manager.</p> <p>Returns:</p> Type Description <code>tuple[runner, context]</code> <p>The runner and an appropriate context manager. - For reusable clients: (client, NoOpAsyncContextManager()) - For fresh instances: (instance, instance)</p>"},{"location":"api/metrics/#leanflow.Metric.run_check_async","title":"<code>run_check_async(*args, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Asynchronously computes the metric for a single example.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments for the metric check.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the metric check.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the metric computation.</p>"},{"location":"api/metrics/#leanflow.BatchMetric","title":"<code>leanflow.BatchMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Base class for metrics that can be computed on a batch of examples.</p>"},{"location":"api/metrics/#leanflow.BatchMetric.compute_batch","title":"<code>compute_batch(examples)</code>","text":"<p>Synchronously computes the metric for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[Any]</code> <p>A list of examples to process.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the batch computation.</p> <p>Raises:</p> Type Description <code>LeanValueError</code> <p>If <code>examples</code> is not a list.</p>"},{"location":"api/metrics/#leanflow.BatchMetric.run_batch_async","title":"<code>run_batch_async(examples)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Asynchronously computes the metric for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[Any]</code> <p>List of examples to process.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the metric computation.</p>"},{"location":"api/metrics/#interactive-metrics","title":"Interactive Metrics","text":"<p>These metrics run on individual examples and typically rely on the Lean REPL to verify logic or proofs.</p> TypeCheckBEqPlusBEqLEquivRfl"},{"location":"api/metrics/#leanflow.TypeCheck","title":"<code>leanflow.TypeCheck</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Runs a Lean statement through the REPL and checks for errors during compilation. Returns True when there are no errors returned from Lean. </p> Example <pre><code>from leanflow import TypeCheck\n\nmetric = TypeCheck(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(\"theorem test : 1 + 1 = 2 := rfl\")\nprint(result)\n</code></pre>"},{"location":"api/metrics/#leanflow.TypeCheck.__init__","title":"<code>__init__(metric_config={}, repl_config={}, client=None, **shared_dependencies)</code>","text":"<p>Initializes the TypeCheck metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary.</p> <code>{}</code> <code>repl_config</code> <code>dict[str, Any]</code> <p>Configuration for the REPL (e.g. {'lean_version': '4.21.0'}).</p> <code>{}</code> <code>client</code> <code>Optional[Client]</code> <p>An existing Client instance to use.</p> <code>None</code> <code>**shared_dependencies</code> <code>Any</code> <p>Other shared dependencies.</p> <code>{}</code>"},{"location":"api/metrics/#leanflow.TypeCheck.run_check_async","title":"<code>run_check_async(statement, header=None)</code>  <code>async</code>","text":"<p>Typechecks a Lean statement.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The Lean code to typecheck.</p> required <code>header</code> <code>Optional[str]</code> <p>Optional header (imports, etc.) to prepend.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the statement typechecks without errors, False otherwise.</p>"},{"location":"api/metrics/#leanflow.BEqPlus","title":"<code>leanflow.BEqPlus</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the BEq+ metric by checking for bidirectional provability using a variety of tactics. Source: Reliable Evaluation and Benchmarks for Statement Autoformalization (Poiroux et al., EMNLP 2025).</p> Example <pre><code>from leanflow import BEqPlus\n\nthm1 = \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\"\nthm2 = \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\"\n\nmetric = BEqPlus(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(thm1, thm2, header=\"import Mathlib\")\nprint(result)\n</code></pre>"},{"location":"api/metrics/#leanflow.BEqPlus.__init__","title":"<code>__init__(metric_config={}, **shared_dependencies)</code>","text":"<p>Initializes the BEqPlus metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary.</p> <code>{}</code> <code>**shared_dependencies</code> <code>Any</code> <p>Shared dependencies. Must include 'repl_config' or 'client'.</p> <code>{}</code>"},{"location":"api/metrics/#leanflow.BEqPlus.run_check_async","title":"<code>run_check_async(statement_1, statement_2, header=None)</code>  <code>async</code>","text":"<p>Computes the BEq+ equivalence for the given example.</p> <p>Parameters:</p> Name Type Description Default <code>statement_1</code> <code>str</code> <p>The first Lean statement.</p> required <code>statement_2</code> <code>str</code> <p>The second Lean statement.</p> required <code>header</code> <code>Optional[str]</code> <p>Optional header (imports, etc.) to prepend.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if both statements can prove each other, False otherwise.</p>"},{"location":"api/metrics/#leanflow.BEqL","title":"<code>leanflow.BEqL</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the BEq-L metric by checking for bidirectional equivalence using <code>exact?</code>. Source: Reliable Evaluation and Benchmarks for Statement Autoformalization (Poiroux et al., EMNLP 2025)</p> Example <pre><code>from leanflow import BEqL\n\nthm1 = \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\"\nthm2 = \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\"\n\nmetric = BEqL(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(thm1, thm2)\nprint(result)\n</code></pre>"},{"location":"api/metrics/#leanflow.BEqL.__init__","title":"<code>__init__(metric_config={}, **shared_dependencies)</code>","text":"<p>Initializes the BEqL metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary.</p> <code>{}</code> <code>**shared_dependencies</code> <code>Any</code> <p>Shared dependencies. Must include 'repl_config' or 'client'.</p> <code>{}</code>"},{"location":"api/metrics/#leanflow.BEqL.run_check_async","title":"<code>run_check_async(statement_1, statement_2, header=None)</code>  <code>async</code>","text":"<p>Checks if statement_1 and statement_2 are equivalent using <code>exact?</code>.</p> <p>Parameters:</p> Name Type Description Default <code>statement_1</code> <code>str</code> <p>The first Lean statement.</p> required <code>statement_2</code> <code>str</code> <p>The second Lean statement.</p> required <code>header</code> <code>Optional[str]</code> <p>Optional header (imports, etc.) to prepend.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if both statements can prove each other using <code>exact?</code>, False otherwise.</p>"},{"location":"api/metrics/#leanflow.EquivRfl","title":"<code>leanflow.EquivRfl</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes equivalence by checking for definitional equality using <code>rfl</code>. Source: Conjecturing: An Overlooked Step in Formal Mathematical Reasoning (Sivakumar et al., ArXiv 2025)</p> Example <pre><code>from leanflow import EquivRfl\n\nconjecture_1 = \"abbrev foo : Nat := 2\"\nconjecture_2 = \"abbrev bar : Nat := 1 + 1\"\n\nmetric = EquivRfl(repl_config={\"lean_version\": \"4.24.0\"})\nresult = metric.compute(conjecture_1, conjecture_2)\nprint(result)\n</code></pre>"},{"location":"api/metrics/#leanflow.EquivRfl.__init__","title":"<code>__init__(metric_config={}, **shared_dependencies)</code>","text":"<p>Initializes the EquivRfl metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary.</p> <code>{}</code> <code>**shared_dependencies</code> <code>Any</code> <p>Shared dependencies. Must include 'repl_config' or 'client'.</p> <code>{}</code>"},{"location":"api/metrics/#leanflow.EquivRfl.run_check_async","title":"<code>run_check_async(statement_1, statement_2, header=None)</code>  <code>async</code>","text":"<p>Checks for definitional equality between two statements.</p> <p>Parameters:</p> Name Type Description Default <code>statement_1</code> <code>str</code> <p>The first Lean statement.</p> required <code>statement_2</code> <code>str</code> <p>The second Lean statement.</p> required <code>header</code> <code>Optional[str]</code> <p>Optional header (imports, etc.) to prepend.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the statements are definitionally equal, False otherwise.</p>"},{"location":"api/metrics/#llm-batch-metrics","title":"LLM &amp; Batch Metrics","text":"<p>These metrics leverage LLMs to perform semantic evaluations. They are designed to process inputs in batches.</p> LLMGraderBEqConJudge"},{"location":"api/metrics/#leanflow.LLMGrader","title":"<code>leanflow.LLMGrader</code>","text":"<p>               Bases: <code>LLMAsAJudge</code></p> <p>Computes semantic equivalence using a LLM-as-a-judge. Source: FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models (Yu et al., ArXiv 2025).</p> <p>This metric performs back-translation of formal statements to natural language using the backtranslation model and then uses the comparison model to compare the semantic equivalence of the back-translated statements.</p> Example <pre><code>from leanflow import LLMGrader\n\ndata = {\n    \"formal_statement\": \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\",\n    \"formal_statement_generated\": \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\"\n}\n\nmetric = LLMGrader(\n    api_config={\"base_url\": \"&lt;URL&gt;\", \"api_key\": \"&lt;KEY&gt;\"},\n    backtranslation={\"model\": \"deepseek-math\"},\n    comparison={\"model\": \"gpt-4\"},\n)\n\nresult = metric.compute_batch([data])\nprint(result)\n</code></pre>"},{"location":"api/metrics/#leanflow.LLMGrader.__init__","title":"<code>__init__(metric_config={}, **shared_dependencies)</code>","text":"<p>Initializes the LLMGrader metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary.</p> <code>{}</code> <code>**shared_dependencies</code> <code>Any</code> <p>Shared dependencies.</p> <code>{}</code> <p>Raises:</p> Type Description <code>LeanEnvironmentError</code> <p>If <code>use_vllm</code> is True but vLLM is not installed.</p>"},{"location":"api/metrics/#leanflow.LLMGrader.run_batch_async","title":"<code>run_batch_async(examples)</code>  <code>async</code>","text":"<p>Computes the LLM grader metric for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[dict[str, Any]]</code> <p>List of examples to process.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of results, each containing 'llm_grader' (bool) and raw outputs.</p>"},{"location":"api/metrics/#leanflow.BEq","title":"<code>leanflow.BEq</code>","text":"<p>               Bases: <code>BatchMetric</code></p> <p>Computes the BEq metric using a heuristic check followed by LLM-based tactic generation. Source: Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach (Liu et al., 2024)</p> <p>First tries a heuristic <code>exact?</code> check. If that fails, it uses an LLM in a loop to generate and validate tactics to prove equivalence.</p> Example <pre><code>from leanflow import BEq\n\ndata = {\n    \"header\": \"import Mathlib\"\n    \"formal_statement\": \"theorem t1 (a b c : Prop) : a \u2227 b \u2192 c := by sorry\",\n    \"formal_statement_generated\": \"theorem t2 (a b c : Prop) : a \u2192 b \u2192 c := by sorry\",\n}\n\nmetric = BEq(\n    api_config={\"base_url\": \"&lt;URL&gt;\", \"api_key\": \"&lt;KEY&gt;\"},\n    tactic_generator={\"model\": \"deepseek-math\"},\n    repl_config={\"lean_version\": \"v4.24.0\"}\n)\n\nresult = metric.compute_batch([data])\nprint(result)\n</code></pre>"},{"location":"api/metrics/#leanflow.BEq.__init__","title":"<code>__init__(metric_config={}, **shared_dependencies)</code>","text":"<p>Initializes the BEq metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary.</p> <code>{}</code> <code>**shared_dependencies</code> <code>Any</code> <p>Shared dependencies. Must include 'repl_config' or 'client'.</p> <code>{}</code> <p>Raises:</p> Type Description <code>LeanValueError</code> <p>If neither 'repl_config' nor 'client' is provided, or if 'tactic_generator.model' is missing.</p>"},{"location":"api/metrics/#leanflow.BEq.run_batch_async","title":"<code>run_batch_async(examples)</code>  <code>async</code>","text":"<p>Computes the BEq metric for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[dict[str, Any]]</code> <p>List of examples to process.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of results, each containing 'beq' (bool) and 'generated_tactics'.</p>"},{"location":"api/metrics/#leanflow.ConJudge","title":"<code>leanflow.ConJudge</code>","text":"<p>               Bases: <code>LLMAsAJudge</code></p> <p>Judges if a generated Lean statement correctly incorporates a given conjecture using an LLM. Source: Conjecturing: An Overlooked Step in Formal Mathematical Reasoning (Sivakumar et al., ArXiv 2025)</p> Example <pre><code>from leanflow import ConJudge\n\ndata = {\n    \"header\": \"import Mathlib\",\n    \"formal_conjecture\": \"abbrev conjecture : \u2115 : 13\",\n    \"formal_statement\": \"theorem hackmath_4 : IsLeast {n | \u2200 f : Fin n \u2192 Fin 12, \u2203 a b, f a = f b} ((conjecture) : \u2115 ) := by sorry\",\n    \"formal_statement_generated\": \"theorem hackmath_4 : IsLeast {n | \u2200 f : Fin n \u2192 Fin 12, \u2203 a b, f a = f b} (26 / 2 : \u2115 ) := by sorry\",\n}\n\nmetric = ConJudge(\n    api_config={\"base_url\": \"&lt;URL&gt;\", \"api_key\": \"&lt;KEY&gt;\"},\n    comparison={\"model\": \"gpt-4\"}\n)\n\nresult = metric.compute_batch([data])\nprint(result)\n</code></pre>"},{"location":"api/metrics/#leanflow.ConJudge.__init__","title":"<code>__init__(metric_config={}, **shared_dependencies)</code>","text":"<p>Initializes the ConJudge metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary.</p> <code>{}</code> <code>**shared_dependencies</code> <code>Any</code> <p>Shared dependencies.</p> <code>{}</code> <p>Raises:</p> Type Description <code>LeanEnvironmentError</code> <p>If <code>use_vllm</code> is True but vLLM is not installed.</p>"},{"location":"api/metrics/#leanflow.ConJudge.run_batch_async","title":"<code>run_batch_async(examples)</code>  <code>async</code>","text":"<p>Computes the ConJudge metric for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[dict[str, Any]]</code> <p>List of examples to process.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of results, each containing 'conjudge' (bool) and raw outputs.</p>"},{"location":"api/repl/","title":"REPL API Reference","text":"<p>The REPL module is the core of LeanFlow. It manages the lifecycle of local Lean 4 processes, handles toolchain management (via elan), and provides methods to execute code and query the proof state.</p> <p>It offers two interfaces:</p> <ul> <li> <p><code>SyncREPL</code>: A blocking wrapper, ideal for scripts and notebooks.</p> </li> <li> <p><code>REPL</code>: The native asynchronous implementation, ideal for high-concurrency workloads.</p> </li> </ul> SynchronousAsynchronous <p><code>SyncREPL</code> is a wrapper around <code>REPL</code> abstracting the asyncio event loop.</p>"},{"location":"api/repl/#leanflow.SyncREPL","title":"<code>leanflow.SyncREPL</code>","text":"<p>Synchronous wrapper around REPL for convenience.</p> <p>Provides a blocking API for interacting with the Lean REPL without needing to use async/await.</p> Example <pre><code>from leanflow import SyncREPL\n\nwith SyncREPL(lean_version=\"v4.24.0\") as repl:\n    result = repl.run(\"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\")\n    print(result)\n</code></pre>"},{"location":"api/repl/#leanflow.SyncREPL.__init__","title":"<code>__init__(lean_version=None, repl_path=None, project_path=None, **kwargs)</code>","text":"<p>Initialize the SyncREPL with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>lean_version</code> <code>Optional[str]</code> <p>The Lean version to use (e.g. \"4.24.0\").</p> <code>None</code> <code>repl_path</code> <code>Optional[Path]</code> <p>Path to the Lean REPL executable.</p> <code>None</code> <code>project_path</code> <code>Optional[Path]</code> <p>Path to a local Lean project.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Other arguments passed to REPL (timeout, max_memory_mb).</p> <code>{}</code>"},{"location":"api/repl/#leanflow.SyncREPL.start","title":"<code>start()</code>","text":"<p>Start the REPL process.</p> <p>This is called automatically when using the context manager.</p>"},{"location":"api/repl/#leanflow.SyncREPL.run","title":"<code>run(command, env=None)</code>","text":"<p>Run a Lean command synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>A single command string.</p> required <code>env</code> <code>Optional[Union[Environment, int]]</code> <p>Optional environment to run in.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>Environment or LeanError.</p>"},{"location":"api/repl/#leanflow.SyncREPL.run_list","title":"<code>run_list(commands, env=None)</code>","text":"<p>Run Lean commands synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>commands</code> <code>list[str]</code> <p>A list of command strings.</p> required <code>env</code> <code>Optional[Union[Environment, int]]</code> <p>Optional environment to run in.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Union[Environment, LeanError]]</code> <p>List of results (Environment or LeanError).</p>"},{"location":"api/repl/#leanflow.SyncREPL.run_file","title":"<code>run_file(path, return_all_states=False)</code>","text":"<p>Run a Lean file synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the Lean file.</p> required <code>return_all_states</code> <code>bool</code> <p>Whether to return all intermediate states.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>Environment or LeanError.</p>"},{"location":"api/repl/#leanflow.SyncREPL.run_tactic","title":"<code>run_tactic(tactic, state)</code>","text":"<p>Run a tactic in a proof state synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>tactic</code> <code>str</code> <p>The tactic to run.</p> required <code>state</code> <code>ProofState</code> <p>The current proof state.</p> required <p>Returns:</p> Type Description <code>Union[ProofState, LeanError]</code> <p>New ProofState or LeanError.</p>"},{"location":"api/repl/#leanflow.SyncREPL.close","title":"<code>close()</code>","text":"<p>Close the REPL process and clean up resources.</p> <p>This is called automatically when using the context manager.</p>"},{"location":"api/repl/#leanflow.REPL","title":"<code>leanflow.REPL</code>","text":"<p>REPL is an asynchronous, robust wrapper for the Lean REPL (https://github.com/leanprover-community/repl). It automatically manages its memory and restarts itself.</p> <p>Run as an async context manager (<code>async with</code>) or to manually call <code>close_async</code> when done to ensure proper cleanup.</p> Example <pre><code>import asyncio\nfrom leanflow import REPL\n\nasync def main():\n    async with REPL(lean_version=\"4.24.0\") as repl:\n        result = await repl.run(\"theorem add_zero_nat (n : Nat) : n + 0 = n := by sorry\")\n        print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/repl/#leanflow.REPL.__init__","title":"<code>__init__(lean_version=None, repl_path=None, project_path=None, timeout=300, header=None, dacite_config=None, fail_on_header_error=True, log_dir=None, log_level=None, debug=False, manage_memory=True, max_memory_mb=8192, max_restart_attempts=3, **kwargs)</code>","text":"<p>Initializes the REPL.</p> <p>Parameters:</p> Name Type Description Default <code>lean_version</code> <code>Optional[str]</code> <p>The Lean version to use (e.g. \"4.21.0\").</p> <code>None</code> <code>repl_path</code> <code>Optional[Path]</code> <p>Path to the Lean REPL executable.</p> <code>None</code> <code>project_path</code> <code>Optional[Path]</code> <p>Path to the Lean project directory.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout for the REPL process.</p> <code>300</code> <code>header</code> <code>Optional[str]</code> <p>Header to be sent to the REPL.</p> <code>None</code> <code>dacite_config</code> <code>Optional[Config]</code> <p>Configuration for dacite.</p> <code>None</code> <code>fail_on_header_error</code> <code>bool</code> <p>If True, raises LeanHeaderError when header execution fails. If False, log the error and continue with base_env_id=None.</p> <code>True</code> <code>log_dir</code> <code>Optional[Union[str, Path]]</code> <p>Explicit directory to save log files.</p> <code>None</code> <code>log_level</code> <code>Optional[str]</code> <p>The logging level (e.g., \"INFO\", \"DEBUG\").</p> <code>None</code> <code>debug</code> <code>bool</code> <p>If True, enables file logging even if log_dir is not provided.</p> <code>False</code> <code>manage_memory</code> <code>bool</code> <p>If True, manages memory and restarts if needed.</p> <code>True</code> <code>max_memory_mb</code> <code>int</code> <p>Maximum memory in MB before a restart is triggered.</p> <code>8192</code> <code>max_restart_attempts</code> <code>int</code> <p>Maximum consecutive restarts before raising MemoryError.</p> <code>3</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the EnvironmentManager.</p> <code>{}</code>"},{"location":"api/repl/#leanflow.REPL.load_from_yaml","title":"<code>load_from_yaml(yaml_path)</code>  <code>classmethod</code>","text":"<p>Loads a REPL instance from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>Path</code> <p>Path to the YAML file.</p> required <p>Returns:</p> Type Description <code>REPL</code> <p>The loaded REPL instance.</p>"},{"location":"api/repl/#leanflow.REPL.run","title":"<code>run(command, env=None)</code>  <code>async</code>","text":"<p>Runs a Lean command in the specified environment.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The Lean command to execute.</p> required <code>env</code> <code>Optional[Union[Environment, int]]</code> <p>The environment to run in.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>The resulting environment or an error.</p>"},{"location":"api/repl/#leanflow.REPL.run_list","title":"<code>run_list(commands, env=None)</code>  <code>async</code>","text":"<p>Runs a list of commands in the specified environment.</p> <p>Parameters:</p> Name Type Description Default <code>commands</code> <code>list[str]</code> <p>A list of command strings.</p> required <code>env</code> <code>Optional[Union[Environment, int]]</code> <p>The Lean environment.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Union[Environment, LeanError]]</code> <p>List of results.</p>"},{"location":"api/repl/#leanflow.REPL.run_file_async","title":"<code>run_file_async(path, return_all_states=False)</code>  <code>async</code>","text":"<p>Runs a Lean file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the Lean file.</p> required <code>return_all_states</code> <code>bool</code> <p>Whether to return all intermediate states (not fully supported in return type yet).</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>The resulting environment or an error.</p>"},{"location":"api/repl/#leanflow.REPL.pickle_env_async","title":"<code>pickle_env_async(path, env)</code>  <code>async</code>","text":"<p>Pickles an environment to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Destination path.</p> required <code>env</code> <code>Environment</code> <p>Environment to pickle.</p> required <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>Result or error.</p>"},{"location":"api/repl/#leanflow.REPL.unpickle_env_async","title":"<code>unpickle_env_async(path)</code>  <code>async</code>","text":"<p>Unpickles an environment from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Source path.</p> required <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>The unpickled environment or error.</p>"},{"location":"api/repl/#leanflow.REPL.pickle_state_async","title":"<code>pickle_state_async(path, state)</code>  <code>async</code>","text":"<p>Pickles a proof state to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Destination path.</p> required <code>state</code> <code>ProofState</code> <p>Proof state to pickle.</p> required <p>Returns:</p> Type Description <code>Union[ProofState, LeanError]</code> <p>Result or error.</p>"},{"location":"api/repl/#leanflow.REPL.unpickle_state_async","title":"<code>unpickle_state_async(path)</code>  <code>async</code>","text":"<p>Unpickles a proof state from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Source path.</p> required <p>Returns:</p> Type Description <code>Union[ProofState, LeanError]</code> <p>The unpickled proof state or error.</p>"},{"location":"api/repl/#leanflow.REPL.close_async","title":"<code>close_async()</code>  <code>async</code>","text":"<p>Closes the Lean REPL subprocess and its entire process tree.</p>"},{"location":"api/server/","title":"Server API Reference","text":"<p>The Server module provides the backend infrastructure for running LeanFlow as a service. It wraps the Lean REPL in a HTTP server (FastAPI), managing a pool of worker processes to handle concurrent requests.</p>"},{"location":"api/server/#server-class","title":"Server Class","text":"<p>The <code>Server</code> class is the main entry point. It initializes the worker pool, handles the request queue, and manages the lifecycle of persistent Lean environments.</p>"},{"location":"api/server/#leanflow.Server","title":"<code>leanflow.Server</code>","text":"<p>Server that owns and configures the StateManager.</p> Example <pre><code>import asyncio\nfrom leanflow import Server\n\nasync def main():\n    server = await Server.create_from_yaml(\"server.yaml\")\n    try:\n        result = await server.run(\"#eval 1 + 1\")\n        print(result)\n    finally:\n        await server.shutdown()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/server/#leanflow.Server.__init__","title":"<code>__init__(state_manager)</code>","text":"<p>Initialize the server with a StateManager instance.</p> <p>Parameters:</p> Name Type Description Default <code>state_manager</code> <code>StateManager</code> <p>The initialized state manager.</p> required"},{"location":"api/server/#leanflow.Server.create_from_yaml","title":"<code>create_from_yaml(yaml_path)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Asynchronously creates the Server by loading configuration from a YAML file.</p> <p>This method waits for the StateManager to be fully initialized before returning.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>Union[str, Path]</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Type Description <code>Server</code> <p>The initialized Server instance.</p>"},{"location":"api/server/#leanflow.Server.run","title":"<code>run(command, env=None)</code>  <code>async</code>","text":"<p>Run command using the StateManager.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The Lean command to execute.</p> required <code>env</code> <code>Optional[int]</code> <p>The environment ID to run in.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Environment, LeanError]</code> <p>The result of the command.</p>"},{"location":"api/server/#leanflow.Server.delete_environment","title":"<code>delete_environment(env_id)</code>  <code>async</code>","text":"<p>Delete environment using the StateManager.</p> <p>Parameters:</p> Name Type Description Default <code>env_id</code> <code>int</code> <p>The environment ID to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False if not found.</p>"},{"location":"api/server/#leanflow.Server.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Shutdown the server by shutting down the StateManager.</p>"},{"location":"api/server/#starting-the-server","title":"Starting the Server","text":""},{"location":"api/server/#command-line","title":"Command Line","text":"<pre><code>leanflow-serve --config server.yaml\n</code></pre>"},{"location":"api/server/#configuration-file-serveryaml","title":"Configuration File (<code>server.yaml</code>)","text":"<p>The configuration file is split into two sections:</p> <ul> <li> <p><code>server</code>: Controls HTTP server and worker settings.</p> </li> <li> <p><code>repl</code>: Passed directly to the underlying Lean REPL instances.</p> </li> </ul> <pre><code>server:\n  host: 0.0.0.0\n  port: 8000\n  workers: 10\n  stateless: false\n\nrepl:\n  lean_version: \"4.24.0\"\n  require_mathlib: true\n  timeout: 300\n  # Optional: Shared header to run on startup for every worker\n  header: |\n    import Mathlib\n</code></pre>"},{"location":"api/server/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>server.host</code> string <code>localhost</code> Host address to bind to <code>server.port</code> int <code>8000</code> Port number <code>server.workers</code> int <code>10</code> Number of REPL workers <code>server.stateless</code> bool <code>false</code> Stateless mode <code>repl.lean_version</code> string required Lean version to use <code>repl.require_mathlib</code> bool <code>true</code> Whether to include Mathlib <code>repl.timeout</code> int <code>300</code> Command timeout in seconds <code>repl.header</code> string <code>null</code> Commands to run at startup"},{"location":"api/server/#rest-api-endpoints","title":"REST API Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/status</code> Check server status <code>POST</code> <code>/run</code> Execute a Lean command"},{"location":"api/server/#post-run","title":"POST /run","text":"<p>Request: <pre><code>{\"command\": \"#eval 1 + 1\", \"env\": null}\n</code></pre></p> <p>Response: <pre><code>{\n  \"result\": {\n    \"env\": 1,   // ID of the new resulting environment\n    \"messages\": [\n      {\n        \"severity\": \"info\",\n        \"pos\": {\"line\": 1, \"column\": 0},\n        \"data\": \"2\"\n      }\n    ],\n    \"sorries\": [],\n    \"goals\": []\n  }\n}\n</code></pre></p>"}]}